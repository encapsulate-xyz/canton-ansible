canton {
  features {
    enable-preview-commands = yes
    enable-testing-commands = yes
  }
  parameters {
    manual-start = no
    non-standard-config = yes
    # Bumping because our topology state can get very large due to
    # a large number of participants.
    timeouts.processing.verify-active = 40.seconds
    timeouts.processing.slow-future-warn = 20.seconds
    timeouts.processing.sequenced-event-processing-bound = 10m
    startup-memory-check-config {
      reporting-level = "ignore"
    }
  }

  # Bumping because our topology state can get very large due to
  # a large number of participants.
  monitoring.logging.delay-logging-threshold = 40.seconds

  participants {
    participant {
      init = {
          generate-topology-transactions-and-keys = false
          identity.type = manual
      }

      sequencer-client {
        acknowledgement-interval = 10m
        # Use a higher number of in flight batches to increase throughput
        maximum-in-flight-event-batches = 50
      }

      monitoring.grpc-health-server {
        address = "0.0.0.0"
        port = {{ canton.participant.ports.grpc_port }}
      }
      storage {
        type = postgres
        config {
           dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
           properties = {
             serverName = {{ postgres.server }}
             portNumber = {{ postgres.port }}
             databaseName = participant-{{ canton.migration_id }}
             currentSchema = participant
             user = {{ postgres.user }}
             password = "{{ postgres.password }}"
             tcpKeepAlive = true
           }
         }
         parameters {
           migrate-and-start = true
           max-connections = 32
         }
       }

      admin-api {
        address = "0.0.0.0"
        port = {{ canton.participant.ports.admin_port }}
      }

      init {
        ledger-api.max-deduplication-duration = 30s
      }

      ledger-api {
        address = "0.0.0.0"
        port = {{ canton.participant.ports.ledger_port }}
        user-management-service.additional-admin-user-id = {{ canton.admin_user }}
        auth-services = [{
          type = jwt-jwks
          url = "{{ canton.auth_jwks_url }}"

          # TODO(DACH-NY/canton-network-internal#502) Use different audiences per participant.
          target-audience = "{{ canton.auth_target_audience }}"
        }]
        # We need to bump this because we run one stream per user +
        # polling for domain connections which can add up quite a bit
        # once you're around ~100 users.
        rate-limit.max-api-services-queue-size = 80000
        interactive-submission-service {
          enable-verbose-hashing = true
        }
        topology-aware-package-selection {
            enabled = true
        }
      }

      http-ledger-api.server {
        port = {{ canton.participant.ports.http_ledger_port }}
        address = 0.0.0.0
      }

      parameters {
        initial-protocol-version = 33
        # tune the synchronisation protocols contract store cache
        caching {
          contract-store {
            maximum-size = 1000 # default 1e6
            expire-after-access = 120s # default 10 minutes
          }
        }
        # Bump ACS pruning interval to make sure ACS snapshots are available for longer
        journal-garbage-collection-delay = 24h
      }

      # TODO(DACH-NY/canton-network-node#8331) Tune cache sizes
      # from https://docs.daml.com/2.8.0/canton/usermanual/performance.html#configuration
      # tune caching configs of the ledger api server
      ledger-api {
        index-service {
          max-contract-state-cache-size = 1000 # default 1e4
          max-contract-key-state-cache-size = 1000 # default 1e4

          # The in-memory fan-out will serve the transaction streams from memory as they are finalized, rather than
          # using the database. Therefore, you should choose this buffer to be large enough such that the likeliness of
          # applications having to stream transactions from the database is low. Generally, having a 10s buffer is
          # sensible. Therefore, if you expect e.g. a throughput of 20 tx/s, then setting this number to 200 is sensible.
          # The default setting assumes 100 tx/s.
          max-transactions-in-memory-fan-out-buffer-size = 200 # default 1000
        }
        # Restrict the command submission rate (mainly for SV participants, since they are granted unlimited traffic)
        command-service {
          max-commands-in-flight = 30 # default = 256
        }
      }

      topology.broadcast-batch-size = 1
    }
  }
}
